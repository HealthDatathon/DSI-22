{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Detection of Covid Positive Cases using Image Processing**","metadata":{}},{"cell_type":"code","source":"%%HTML\n\n<html>\n    <body>\n        <iframe src = \"https://atharv-chaudhari.github.io/Project-Covid-Cache/\" height = \"650px\" width = \"100%\">\n        </iframe>\n    </body>\n</html>","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T11:28:57.881693Z","iopub.execute_input":"2022-02-21T11:28:57.88204Z","iopub.status.idle":"2022-02-21T11:28:57.906122Z","shell.execute_reply.started":"2022-02-21T11:28:57.88193Z","shell.execute_reply":"2022-02-21T11:28:57.905489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"> COVID-19 tests are currently hard to come by â€” there are simply not enough of them and they cannot be manufactured fast enough, which is causing panic.\n\n> Given that there are limited COVID-19 testing kits, we need to rely on other diagnosis measures.\n\n> We thought to explore X-ray images as doctors frequently use X-rays and CT scans to diagnose pneumonia, lung inflammation, abscesses, and/or enlarged lymph nodes.\n\n> Since COVID-19 attacks the epithelial cells that line our respiratory tract, we can use X-rays to analyze the health of a patientâ€™s lungs.\n\n<div style=\"width:100%;text-align: center;\"> \n    <img align=middle src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/Projectional_rendering_of_CT_scan_of_thorax_%28thumbnail%29.gif\" alt=\"Heat beating\" style=\"height:400px;margin-top:3rem;\"> \n</div>","metadata":{}},{"cell_type":"markdown","source":"# [Data Source](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database)","metadata":{}},{"cell_type":"markdown","source":"# Loading Required Libraries ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Data Reading \n\nimport os\nfrom glob import glob\nfrom PIL import Image\n\n# Data Processing \n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport random\nimport albumentations as A\n\n# Data Analysis\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Modeling & Model Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import image\nfrom tensorflow.keras import layers, models\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, recall_score, accuracy_score, precision_score, f1_score\n\n# Grad-CAM\n\nimport keras\nimport matplotlib.cm as cm","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:28:57.90767Z","iopub.execute_input":"2022-02-21T11:28:57.908144Z","iopub.status.idle":"2022-02-21T11:29:06.180794Z","shell.execute_reply.started":"2022-02-21T11:28:57.908108Z","shell.execute_reply":"2022-02-21T11:29:06.180077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading The Data","metadata":{}},{"cell_type":"code","source":"levels = ['Normal', 'COVID']\npath = \"../input/covid19-radiography-database/COVID-19_Radiography_Dataset\"\ndata_dir = os.path.join(path)\n\ndata = []\nfor id, level in enumerate(levels):\n    for file in os.listdir(os.path.join(data_dir, level)):\n        data.append(['{}/{}'.format(level, file), level])\n\ndata = pd.DataFrame(data, columns = ['image_file', 'corona_result'])\n\ndata['path'] = path + '/' + data['image_file']\ndata['corona_result'] = data['corona_result'].map({'Normal': 'Negative', 'COVID': 'Positive'})\nsamples = 13808\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:29:06.183719Z","iopub.execute_input":"2022-02-21T11:29:06.184263Z","iopub.status.idle":"2022-02-21T11:29:06.689837Z","shell.execute_reply.started":"2022-02-21T11:29:06.184223Z","shell.execute_reply":"2022-02-21T11:29:06.689142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of Duplicated Samples: %d'%(data.duplicated().sum()))\nprint('Number of Total Samples: %d'%(data.isnull().value_counts()))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:29:06.691146Z","iopub.execute_input":"2022-02-21T11:29:06.691558Z","iopub.status.idle":"2022-02-21T11:29:06.715583Z","shell.execute_reply.started":"2022-02-21T11:29:06.69152Z","shell.execute_reply":"2022-02-21T11:29:06.714736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis ","metadata":{}},{"cell_type":"markdown","source":"### 1. Count Plot ","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['corona_result'] = ['Positive', 'Negative']\ndf['Count'] = [len(data[data['corona_result'] == 'Positive']), len(data[data['corona_result'] == 'Negative'])]\ndf = df.sort_values(by = ['Count'], ascending = False)\n\nfig = px.bar(df, x = 'corona_result', y = 'Count', \n             color = \"corona_result\", text_auto='', width = 600, \n             color_discrete_sequence = [\"orange\", \"purple\"],\n             template = 'plotly_dark')\n\nfig.update_xaxes(showgrid = False)\nfig.update_yaxes(showgrid = False)\nfig.update_traces(textfont_size = 12, textangle = 0, textposition = \"outside\", cliponaxis = False)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:29:06.717968Z","iopub.execute_input":"2022-02-21T11:29:06.718247Z","iopub.status.idle":"2022-02-21T11:29:07.357498Z","shell.execute_reply.started":"2022-02-21T11:29:06.718212Z","shell.execute_reply":"2022-02-21T11:29:07.356844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Image Samples","metadata":{}},{"cell_type":"code","source":"data['image'] = data['path'].map(lambda x: np.asarray(Image.open(x).resize((75,75))))\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:29:07.358922Z","iopub.execute_input":"2022-02-21T11:29:07.359388Z","iopub.status.idle":"2022-02-21T11:31:04.464603Z","shell.execute_reply.started":"2022-02-21T11:29:07.359351Z","shell.execute_reply":"2022-02-21T11:31:04.463895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_samples = 3\n\nfig, m_axs = plt.subplots(2, n_samples, figsize = (6*n_samples, 3*4))\n\nfor n_axs, (type_name, type_rows) in zip(m_axs, data.sort_values(['corona_result']).groupby('corona_result')):\n    n_axs[1].set_title(type_name, fontsize = 15)\n    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state = 1234).iterrows()):       \n        picture = c_row['path']\n        image = cv2.imread(picture)\n        c_ax.imshow(image)\n        c_ax.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:04.465745Z","iopub.execute_input":"2022-02-21T11:31:04.466282Z","iopub.status.idle":"2022-02-21T11:31:05.331496Z","shell.execute_reply.started":"2022-02-21T11:31:04.466249Z","shell.execute_reply":"2022-02-21T11:31:05.328101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Random Image Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure()\nimage = cv2.imread(\"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/COVID-1002.png\")\nplt.imshow(image)\nplt.axis('off')\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:05.332457Z","iopub.execute_input":"2022-02-21T11:31:05.332676Z","iopub.status.idle":"2022-02-21T11:31:05.449037Z","shell.execute_reply.started":"2022-02-21T11:31:05.332647Z","shell.execute_reply":"2022-02-21T11:31:05.448342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Image Shape: {}'.format(image.shape))\nprint('Image Height: {}'.format(image.shape[0]))\nprint('Image Width: {}'.format(image.shape[1]))\nprint('Image Dimension: {}'.format(image.ndim))\nprint('Image Size: {}kb'.format(image.size//1024))\nprint('Image Data Type: {}'.format(image.dtype))\nprint('Maximum RGB value of the image: {}'.format(image.max()))\nprint('Minimum RGB value of the image: {}'.format(image.min()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T11:31:05.450568Z","iopub.execute_input":"2022-02-21T11:31:05.45104Z","iopub.status.idle":"2022-02-21T11:31:05.463357Z","shell.execute_reply.started":"2022-02-21T11:31:05.450989Z","shell.execute_reply":"2022-02-21T11:31:05.462444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We observe that the image has 3 channels, hence it in in RGB scale even if these are X-ray images.","metadata":{}},{"cell_type":"markdown","source":"### 4. B-Channel","metadata":{}},{"cell_type":"code","source":"plt.title('B channel', fontsize = 14)\nplt.imshow(image[ : , : , 0])\nplt.axis('off');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:05.465157Z","iopub.execute_input":"2022-02-21T11:31:05.465629Z","iopub.status.idle":"2022-02-21T11:31:05.60408Z","shell.execute_reply.started":"2022-02-21T11:31:05.465592Z","shell.execute_reply":"2022-02-21T11:31:05.603376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Ben Graham's Method","metadata":{}},{"cell_type":"markdown","source":"> First, we convert the images to greyscale and then apply Gaussian blur to them.","metadata":{}},{"cell_type":"code","source":"all_covid = []\nall_normal = []\n\nall_normal.extend(glob(os.path.join(path, \"Normal/*.png\")))\nall_covid.extend(glob(os.path.join(path, \"COVID/*.png\")))\n\nrandom.shuffle(all_normal)\nrandom.shuffle(all_covid)\n\nimages = all_normal[:50] + all_covid[:50]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:05.607704Z","iopub.execute_input":"2022-02-21T11:31:05.608147Z","iopub.status.idle":"2022-02-21T11:31:05.681045Z","shell.execute_reply.started":"2022-02-21T11:31:05.608112Z","shell.execute_reply":"2022-02-21T11:31:05.68041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (18, 7))\nfig.suptitle(\"Ben Grahamns Method of Analysis\", fontsize = 15)\ncolumns = 4; rows = 2\n\nfor i in range(1, columns*rows +1):\n    img = cv2.imread(images[i])\n    img = cv2.resize(img, (512, 512))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    img = cv2.addWeighted (img, 4, cv2.GaussianBlur(img, (0,0), 512/10), -4, 128)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\n    plt.axis(False)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:05.682135Z","iopub.execute_input":"2022-02-21T11:31:05.682368Z","iopub.status.idle":"2022-02-21T11:31:10.296435Z","shell.execute_reply.started":"2022-02-21T11:31:05.682335Z","shell.execute_reply":"2022-02-21T11:31:10.295801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Albumentations Visualization","metadata":{}},{"cell_type":"code","source":"def plot_multiple_img(img_matrix_list, title_list, ncols, main_title = \"\"):\n    \n    fig, myaxes = plt.subplots(figsize = (15, 8), nrows = 2, ncols = ncols, squeeze = False)\n    fig.suptitle(main_title, fontsize = 18)\n    fig.subplots_adjust(wspace = 0.3)\n    fig.subplots_adjust(hspace = 0.3)\n    \n    for i, (img, title) in enumerate(zip(img_matrix_list, title_list)):\n        myaxes[i // ncols][i % ncols].imshow(img)\n        myaxes[i // ncols][i % ncols].set_title(title, fontsize = 15)\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:10.297511Z","iopub.execute_input":"2022-02-21T11:31:10.297857Z","iopub.status.idle":"2022-02-21T11:31:10.309975Z","shell.execute_reply.started":"2022-02-21T11:31:10.297826Z","shell.execute_reply":"2022-02-21T11:31:10.309122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chosen_image = cv2.imread(\"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/COVID-1002.png\")\n\nalbumentation_list = [A.RandomFog(p = 1), A.RandomBrightness(p = 1),\n                      A.RandomCrop(p = 1,height = 199, width = 199), A.Rotate(p = 1, limit = 90),\n                      A.RGBShift(p = 1), A.VerticalFlip(p = 1), A.RandomContrast(limit = 0.5, p = 1)]\n\nimg_matrix_list = []\nbboxes_list = []\nfor aug_type in albumentation_list:\n    img = aug_type(image = chosen_image)['image']\n    img_matrix_list.append(img)\n\nimg_matrix_list.insert(0,chosen_image)    \n\ntitles_list = [\"Original\", \"RandomFog\", \"RandomBrightness\", \"RandomCrop\", \"Rotate\", \"RGBShift\", \"VerticalFlip\", \"RandomContrast\"]\n\nplot_multiple_img(img_matrix_list, titles_list, ncols = 4, main_title = \"Different Types of Augmentations\")","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:10.310908Z","iopub.execute_input":"2022-02-21T11:31:10.311122Z","iopub.status.idle":"2022-02-21T11:31:11.390544Z","shell.execute_reply.started":"2022-02-21T11:31:10.311099Z","shell.execute_reply":"2022-02-21T11:31:11.389851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Image Value Distribution","metadata":{}},{"cell_type":"code","source":"mean_val = []\nstd_dev_val = []\nmax_val = []\nmin_val = []\n\nfor i in range(0, samples):\n    mean_val.append(data['image'][i].mean())\n    std_dev_val.append(np.std(data['image'][i]))\n    max_val.append(data['image'][i].max())\n    min_val.append(data['image'][i].min())\n\nimageEDA = data.loc[:,['image','corona_result','path']]\nimageEDA['mean'] = mean_val\nimageEDA['stedev'] = std_dev_val\nimageEDA['max'] = max_val\nimageEDA['min'] = min_val\n\nimageEDA['subt_mean'] = imageEDA['mean'].mean() - imageEDA['mean']","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:11.39194Z","iopub.execute_input":"2022-02-21T11:31:11.392372Z","iopub.status.idle":"2022-02-21T11:31:12.96872Z","shell.execute_reply.started":"2022-02-21T11:31:11.392338Z","shell.execute_reply":"2022-02-21T11:31:12.967944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax1 = sns.displot(data = imageEDA, x = 'mean', kind=\"kde\", hue = 'corona_result');\nplt.title('Images Colour Mean Value Distribution by Class\\n', fontsize = 12);\n\nax2 = sns.displot(data = imageEDA, x = 'max', kind=\"kde\", hue = 'corona_result');\nplt.title('\\nImages Colour Max Value Distribution by Class\\n', fontsize = 12);\n\nax3 = sns.displot(data = imageEDA, x = 'min', kind=\"kde\", hue = 'corona_result');\nplt.title('\\nImages Colour Min Value Distribution by Class\\n', fontsize = 12);","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:12.970273Z","iopub.execute_input":"2022-02-21T11:31:12.97055Z","iopub.status.idle":"2022-02-21T11:31:14.426429Z","shell.execute_reply.started":"2022-02-21T11:31:12.970512Z","shell.execute_reply":"2022-02-21T11:31:14.425173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The Mean vs Density plot insights for pixels:\n> 1. The max pixel value for Covid Negative cases is greater than 0.014 and less than 0.016.\n> 2. The max pixel value for Covid Positive cases is greater than 0.004 & less than 0.006.\n\n> The Max vs Density plot insights for pixels:\n> 1. The max pixel value for Covid Negative cases is greater than 0.035 and less than 0.040.\n> 2. The max pixel value for Covid Positive cases is 0.005.\n\n> The Min vs Density plot insights for pixels:\n> 1. The max pixel value for Covid Negative cases is greater than 0.4.\n> 2. The max pixel value for Covid Positive cases is greater than 0.0 and less than 0.1.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20, 8))\nsns.set(style = \"ticks\", font_scale = 1)\nax = sns.scatterplot(data = imageEDA, x = \"mean\", y = imageEDA['stedev'], hue = 'corona_result', alpha = 0.8);\nsns.despine(top = True, right = True, left = False, bottom = False)\nplt.xticks(rotation = 0, fontsize = 12)\nax.set_xlabel('\\nImage Channel Colour Mean', fontsize = 14)\nax.set_ylabel('Image Channel Colour Standard Deviation', fontsize = 14)\nplt.title('Mean and Standard Deviation of Image Samples', fontsize = 16);","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:14.427676Z","iopub.execute_input":"2022-02-21T11:31:14.428111Z","iopub.status.idle":"2022-02-21T11:31:15.443537Z","shell.execute_reply.started":"2022-02-21T11:31:14.428075Z","shell.execute_reply":"2022-02-21T11:31:15.442927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We observe that there are 2 clusters formed, one for Covid Positive, one for Covid Negative and both have several overlappings. Overlapping Color Mean range: (100 - 175)\n\n> We observe that for pixels having Std Deviation below 30 are all Covid Positive Images (Orange Colored).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8));\ng = sns.FacetGrid(imageEDA, col = \"corona_result\", height = 5);\ng.map_dataframe(sns.scatterplot, x = 'mean', y = 'stedev');\ng.set_titles(col_template = \"{col_name}\", row_template= \"{row_name}\", size = 12);\ng.fig.subplots_adjust(top = .7);\ng.fig.suptitle('Mean and Standard Deviation of Image Samples', fontsize = 15);\naxes = g.axes.flatten();\naxes[0].set_ylabel('Standard Deviation');\nfor ax in axes:\n    ax.set_xlabel('\\nMean');\ng.fig.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:15.444847Z","iopub.execute_input":"2022-02-21T11:31:15.445338Z","iopub.status.idle":"2022-02-21T11:31:15.972441Z","shell.execute_reply.started":"2022-02-21T11:31:15.445299Z","shell.execute_reply":"2022-02-21T11:31:15.971774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Comparing both Scatter plots, we observe that Postivie Samples have outliers (pixel points).","metadata":{}},{"cell_type":"markdown","source":"### 8. Self Insights ","metadata":{}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/Atharv-Chaudhari/Project-Covid/main/Phase%202%20Deep%20Learning/Images/Insights.png)","metadata":{}},{"cell_type":"markdown","source":"# Data Modeling ","metadata":{}},{"cell_type":"markdown","source":"### Train Test Split ","metadata":{}},{"cell_type":"code","source":"all_data = []\n\n# Storing images and their labels into a list for further Train Test split\n\nfor i in range(len(data)):\n    image = cv2.imread(data['path'][i])\n    image = cv2.resize(image, (70, 70)) / 255.0\n    label = 1 if data['corona_result'][i] == \"Positive\" else 0\n    all_data.append([image, label])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:15.973548Z","iopub.execute_input":"2022-02-21T11:31:15.97409Z","iopub.status.idle":"2022-02-21T11:31:49.210152Z","shell.execute_reply.started":"2022-02-21T11:31:15.974053Z","shell.execute_reply":"2022-02-21T11:31:49.209208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = []\ny = []\n\nfor image, label in all_data:\n    x.append(image)\n    y.append(label)\n\n# Converting to Numpy Array    \nx = np.array(x)\ny = np.array(y)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42)\n\nprint(x_train.shape, x_test.shape, x_val.shape, y_train.shape, y_test.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:49.213328Z","iopub.execute_input":"2022-02-21T11:31:49.213547Z","iopub.status.idle":"2022-02-21T11:31:50.543034Z","shell.execute_reply.started":"2022-02-21T11:31:49.213521Z","shell.execute_reply":"2022-02-21T11:31:50.542208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN Model","metadata":{}},{"cell_type":"code","source":"cnn_model = models.Sequential()\ncnn_model.add(layers.Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', input_shape = (70, 70, 3)))\ncnn_model.add(layers.MaxPooling2D((2, 2)))\ncnn_model.add(layers.Dropout(0.3))\n\ncnn_model.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\ncnn_model.add(layers.MaxPooling2D((2, 2)))\ncnn_model.add(layers.Dropout(0.5))\n\ncnn_model.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\ncnn_model.add(layers.Flatten())\ncnn_model.add(layers.Dense(units = 16, activation = 'relu'))\ncnn_model.add(layers.Dropout(0.2))\n\ncnn_model.add(layers.Dense(units = 2))\n\ncnn_model.compile(optimizer = 'adam', \n           loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n           metrics = ['accuracy'])\n\ncnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:31:50.544528Z","iopub.execute_input":"2022-02-21T11:31:50.544772Z","iopub.status.idle":"2022-02-21T11:31:52.835142Z","shell.execute_reply.started":"2022-02-21T11:31:50.544739Z","shell.execute_reply":"2022-02-21T11:31:52.834418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 4)\n\n#tf.random.set_seed(42)\nhistory = cnn_model.fit(x_train, y_train, \n                        epochs = 50, batch_size = 256,  \n                        validation_data = (x_val, y_val), \n                        callbacks = [es])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-21T11:31:52.836275Z","iopub.execute_input":"2022-02-21T11:31:52.836507Z","iopub.status.idle":"2022-02-21T11:33:10.988721Z","shell.execute_reply.started":"2022-02-21T11:31:52.836473Z","shell.execute_reply":"2022-02-21T11:33:10.987941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yp_train = cnn_model.predict(x_train)\nyp_train = np.argmax(yp_train, axis = 1)\n\nyp_val = cnn_model.predict(x_val)\nyp_val = np.argmax(yp_val, axis = 1)\n\nyp_test = cnn_model.predict(x_test)\nyp_test = np.argmax(yp_test, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:10.992599Z","iopub.execute_input":"2022-02-21T11:33:10.992809Z","iopub.status.idle":"2022-02-21T11:33:19.300233Z","shell.execute_reply.started":"2022-02-21T11:33:10.992783Z","shell.execute_reply":"2022-02-21T11:33:19.299488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluation_parametrics(name, y_train, yp_train, y_val, yp_val, y_test, yp_test):\n    \n    print(\"\\n-----------------------------{}-----------------------------\\n\".format(name))\n    \n    cm_train = confusion_matrix(y_train, yp_train)\n    t1 = ConfusionMatrixDisplay(cm_train)\n    s1 = round((cm_train[0,0]/(cm_train[0,0] + cm_train[0,1])),4)\n    \n    print(\"Classification Report for Train Data\\n\")\n    print(classification_report(y_train, yp_train)) \n    print(\"--------------------------------------------------------------------------\")\n    print(\"Recall on Train Data: \", round(recall_score(y_train, yp_train),4))\n    print(\"Specificity on Train Data: \", s1)\n    print(\"Accuracy on Train Data: \", round(accuracy_score(y_train, yp_train),4))\n    print(\"Precision on Train Data: \", round(precision_score(y_train, yp_train),4))\n    print(\"F1 Score on Train Data: \", round(f1_score(y_train, yp_train),4))\n    print(\"--------------------------------------------------------------------------\")\n       \n    cm_val = confusion_matrix(y_val, yp_val)\n    t2 = ConfusionMatrixDisplay(cm_val)\n    s2 = round((cm_val[0,0]/(cm_val[0,0] + cm_val[0,1])),4)\n    \n    print(\"\\nClassification Report for Validation Data\\n\")\n    print(classification_report(y_val, yp_val))   \n    print(\"--------------------------------------------------------------------------\")\n    print(\"Recall on Val Data: \", round(recall_score(y_val, yp_val),4))\n    print(\"Specificity on Val Data: \", s2)\n    print(\"Accuracy on Val Data: \", round(accuracy_score(y_val, yp_val),4))\n    print(\"Precision on Val Data: \", round(precision_score(y_val, yp_val),4))\n    print(\"F1 Score on Val Data: \", round(f1_score(y_val, yp_val),4))\n    print(\"--------------------------------------------------------------------------\")\n\n    cm_test = confusion_matrix(y_test, yp_test)\n    t3 = ConfusionMatrixDisplay(cm_test)\n    s3 = round((cm_test[0,0]/(cm_test[0,0] + cm_test[0,1])),4)\n    \n    print(\"\\nClassification Report for Test Data\\n\")\n    print(classification_report(y_test, yp_test))   \n    print(\"--------------------------------------------------------------------------\")\n    print(\"Recall on Test Data: \", round(recall_score(y_test, yp_test), 4))\n    print(\"Specificity on Test Data: \", s3)\n    print(\"Accuracy on Test Data: \", round(accuracy_score(y_test, yp_test), 4))\n    print(\"Precision on Test Data: \", round(precision_score(y_test, yp_test), 4))\n    print(\"F1 Score Test Data: \", round(f1_score(y_test, yp_test), 4))\n    print(\"--------------------------------------------------------------------------\")\n    \n    t1.plot()\n    t2.plot()   \n    t3.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:19.30161Z","iopub.execute_input":"2022-02-21T11:33:19.301842Z","iopub.status.idle":"2022-02-21T11:33:19.31669Z","shell.execute_reply.started":"2022-02-21T11:33:19.301809Z","shell.execute_reply":"2022-02-21T11:33:19.316004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_parametrics(\"Convolution Neural Network\", y_train, yp_train, y_val, yp_val, y_test, yp_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:19.317911Z","iopub.execute_input":"2022-02-21T11:33:19.31945Z","iopub.status.idle":"2022-02-21T11:33:19.953583Z","shell.execute_reply.started":"2022-02-21T11:33:19.319411Z","shell.execute_reply":"2022-02-21T11:33:19.952876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list all data in history\n\nprint(history.history.keys())","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:19.954999Z","iopub.execute_input":"2022-02-21T11:33:19.955278Z","iopub.status.idle":"2022-02-21T11:33:19.960437Z","shell.execute_reply.started":"2022-02-21T11:33:19.955242Z","shell.execute_reply":"2022-02-21T11:33:19.959603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summarize History for Accuracy\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc = 'lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:19.962191Z","iopub.execute_input":"2022-02-21T11:33:19.9625Z","iopub.status.idle":"2022-02-21T11:33:20.452399Z","shell.execute_reply.started":"2022-02-21T11:33:19.962423Z","shell.execute_reply":"2022-02-21T11:33:20.451725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summarize History for Loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc = 'upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:20.453438Z","iopub.execute_input":"2022-02-21T11:33:20.453979Z","iopub.status.idle":"2022-02-21T11:33:20.627945Z","shell.execute_reply.started":"2022-02-21T11:33:20.453939Z","shell.execute_reply":"2022-02-21T11:33:20.62731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy Loss Graph\n\npd.DataFrame(history.history).plot()\nplt.title('Model Accuracy/Loss')\nplt.ylabel('Accuracy/Loss')\nplt.xlabel('Epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:20.631741Z","iopub.execute_input":"2022-02-21T11:33:20.631929Z","iopub.status.idle":"2022-02-21T11:33:20.835562Z","shell.execute_reply.started":"2022-02-21T11:33:20.631905Z","shell.execute_reply":"2022-02-21T11:33:20.834867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We observe that Train & Validation Accuracy's Curve slightly overlap and same with Loss Curve. \n\n> Hence, Overfitting is avoided, this is possible because of Dropout Regularization & Early Stopping Metrics.","metadata":{}},{"cell_type":"markdown","source":"# Image Analysis using Grad-CAM ","metadata":{}},{"cell_type":"markdown","source":"### Config- Parameters","metadata":{}},{"cell_type":"code","source":"model_builder = keras.applications.xception.Xception\nimg_size = (299, 299)\npreprocess_input = keras.applications.xception.preprocess_input\ndecode_predictions = keras.applications.xception.decode_predictions\nimag = []\n\nlast_conv_layer_name = \"block14_sepconv2_act\"\n\n# Reading 2 Covid & 2 Normal Images for Grad-Cam Analysis\n\nimg_path = [\"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/COVID-1002.png\",\n            \"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/COVID-101.png\",\n            \"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal/Normal-10004.png\",\n           \"../input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal/Normal-10002.png\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:20.836971Z","iopub.execute_input":"2022-02-21T11:33:20.837481Z","iopub.status.idle":"2022-02-21T11:33:20.843136Z","shell.execute_reply.started":"2022-02-21T11:33:20.837441Z","shell.execute_reply":"2022-02-21T11:33:20.842478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grad - CAM Algorithm","metadata":{}},{"cell_type":"code","source":"# To Get Image into numpy array\n\ndef get_img_array(img_path, size):\n    img = keras.preprocessing.image.load_img(img_path, target_size = size) \n    array = keras.preprocessing.image.img_to_array(img) \n    array = np.expand_dims(array, axis = 0)\n    return array\n\n# Top create heatmaps for the samples\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index = None):\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:20.844202Z","iopub.execute_input":"2022-02-21T11:33:20.844518Z","iopub.status.idle":"2022-02-21T11:33:20.856216Z","shell.execute_reply.started":"2022-02-21T11:33:20.844483Z","shell.execute_reply":"2022-02-21T11:33:20.85535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Heatmap ","metadata":{}},{"cell_type":"code","source":"# Storing Heatmap values into list\n\ncovid_noncovid_heatmap = []\n\nfor i in img_path:\n    img_array = preprocess_input(get_img_array(i, size = img_size))\n    model = model_builder(weights = \"imagenet\")\n    model.layers[-1].activation = None\n    preds = model.predict(img_array)\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    covid_noncovid_heatmap.append(heatmap)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:20.857401Z","iopub.execute_input":"2022-02-21T11:33:20.857665Z","iopub.status.idle":"2022-02-21T11:33:29.450817Z","shell.execute_reply.started":"2022-02-21T11:33:20.857631Z","shell.execute_reply":"2022-02-21T11:33:29.450074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a Superimposed Viz","metadata":{}},{"cell_type":"code","source":"# To Display GradCAM output for the samples\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path = \"cam.jpg\", alpha = 0.4):\n    img = keras.preprocessing.image.load_img(img_path)\n    img = keras.preprocessing.image.img_to_array(img)\n\n    heatmap = np.uint8(255 * heatmap)\n\n    jet = cm.get_cmap(\"jet\")\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    superimposed_img.save(cam_path)\n    \n    imag.append(cv2.imread(img_path))\n    imag.append(cv2.imread(\"./cam.jpg\"))\n\n\nfor i in range(len(img_path)):\n    save_and_display_gradcam(img_path[i], covid_noncovid_heatmap[i])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:29.452065Z","iopub.execute_input":"2022-02-21T11:33:29.452304Z","iopub.status.idle":"2022-02-21T11:33:29.511988Z","shell.execute_reply.started":"2022-02-21T11:33:29.452273Z","shell.execute_reply":"2022-02-21T11:33:29.511283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titles_list = [\"Positive-1\",'Positive-1 Grad','Positive-2','Positive-2 Grad','Negative-1','Negative-1 Grad','Negative-2','Negative-2 Grad']\n\nplot_multiple_img(imag, titles_list, ncols = 4, main_title = \"GRAD-CAM COVID-19 Image Analysis\")","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:29.514793Z","iopub.execute_input":"2022-02-21T11:33:29.51499Z","iopub.status.idle":"2022-02-21T11:33:30.376998Z","shell.execute_reply.started":"2022-02-21T11:33:29.514966Z","shell.execute_reply":"2022-02-21T11:33:30.376371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Positive-1 Sample Insight: In its Grad-CAM image on the right mid part of it we can see the blue color highlighted portion which is opacity due to which it belongs to COVID - Positive Category.\n\n> Positive-2 Sample Insight: In its Grad-CAM image on the left bottom part of it we can see the blue green color highlighted portion which is consolidation and no Tree-Bud due to which it belongs to COVID - Positive Category.\n\n> Negative-1 Sample Insight: In its Grad-CAM image we can see the blue color highlighted portion which is between the Cardiac and Diaphragm and no any opacity was detected due to which it belongs to COVID - Negative Category.\n\n> Negative-2 Sample Insight: In its Grad-CAM image we can see the blue color portion which highlights the Trachea and no other opacity was detected due to which it belongs to COVID - Negative Category.","metadata":{}},{"cell_type":"markdown","source":"### Checking with Model","metadata":{}},{"cell_type":"code","source":"# Checking predictions for the above sample images\n\nfor i in img_path:\n    z_img = cv2.imread(i)\n    z_img = cv2.resize(z_img, (70, 70)) / 255.0\n    z_img = z_img.reshape(1, z_img.shape[0], z_img.shape[1], z_img.shape[2])\n    \n    z = cnn_model.predict(z_img)\n    z = np.argmax(z, axis = 1)\n    print(\"Image\", img_path.index(i) + 1, \":\", z)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:30.378161Z","iopub.execute_input":"2022-02-21T11:33:30.378835Z","iopub.status.idle":"2022-02-21T11:33:30.561742Z","shell.execute_reply.started":"2022-02-21T11:33:30.378797Z","shell.execute_reply":"2022-02-21T11:33:30.561046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving Model","metadata":{}},{"cell_type":"code","source":"cnn_model.save('cnn_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T11:33:30.563187Z","iopub.execute_input":"2022-02-21T11:33:30.563432Z","iopub.status.idle":"2022-02-21T11:33:30.605566Z","shell.execute_reply.started":"2022-02-21T11:33:30.563396Z","shell.execute_reply":"2022-02-21T11:33:30.604843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion ","metadata":{}},{"cell_type":"markdown","source":"> 1. Done Exploratory Image Data Analysis on Covid vs Normal Images using various techniques.\n\n> 2. Applied Convolutional Neural Network Model and got good Accuracy and Loss and prevented Overfitting.\n\n> 3. Applied Grad - CAM Analysis on Sample Images and found few insights.","metadata":{}},{"cell_type":"markdown","source":"# Do upvote if you find this Notebook useful... ðŸ˜ƒ ","metadata":{}}]}